"""
å¼•ç”¨æå–å¼•æ“ - ComfyModelCleaner V2.0

å¤šæºå¼•ç”¨æ£€æµ‹ï¼šä»Pythonæ–‡ä»¶ã€é…ç½®æ–‡ä»¶ã€ç¤ºä¾‹jsonã€READMEç­‰æå–æ¨¡å‹å¼•ç”¨ã€‚
"""

import re
import json
import yaml
from pathlib import Path
from typing import Dict, List, Set, Optional, Any
from dataclasses import dataclass

from .utils import get_custom_nodes_dir, safe_file_operation


@dataclass
class ModelReference:
    """æ¨¡å‹å¼•ç”¨ä¿¡æ¯"""
    model_name: str
    source_file: str
    source_type: str  # 'python', 'config', 'workflow', 'documentation'
    line_number: Optional[int]
    context: str
    confidence: float  # 0.0 - 1.0


# å¼•ç”¨æ¨¡å¼å®šä¹‰
REFERENCE_PATTERNS = {
    'exact_file_reference': [
        r'["\']([^"\']*\.(?:safetensors|ckpt|pt|pth|bin|onnx))["\']',
        r'load_checkpoint\(["\']([^"\']+)["\']',
        r'model_path\s*=\s*["\']([^"\']+)["\']',
    ],
    'directory_reference': [
        r'["\']([^"\']*siglip[^"\']*)["\']',
        r'models[/\\]([a-zA-Z_][a-zA-Z0-9_-]+)',
        r'folder_paths\.get_folder_paths\(["\']([^"\']+)["\']',
    ],
    'model_name_patterns': [
        r'model_name\s*=\s*["\']([^"\']+)["\']',
        r'default_model["\']?\s*:\s*["\']([^"\']+)["\']',
        r'ckpt_name["\']?\s*:\s*["\']([^"\']+)["\']',
        r'checkpoint["\']?\s*:\s*["\']([^"\']+)["\']',
    ],
    'segformer_specific': [
        # ä¸“é—¨é’ˆå¯¹segformeræ¨¡å‹çš„æ¨¡å¼
        r'["\']([^"\']*segformer[^"\']*)["\']',
        r'segformer[_-]?([a-zA-Z0-9_-]+)',
        r'model[_-]?type["\']?\s*:\s*["\']([^"\']*segformer[^"\']*)["\']',
        r'model[_-]?id["\']?\s*:\s*["\']([^"\']*segformer[^"\']*)["\']',
    ],
    'model_identifier_patterns': [
        # æ›´ç²¾ç¡®çš„æ¨¡å‹æ ‡è¯†ç¬¦æ¨¡å¼ - åªåŒ¹é…æ˜ç¡®çš„æ¨¡å‹ç›¸å…³ä¸Šä¸‹æ–‡
        r'model[_-]?(?:type|id|name)["\']?\s*:\s*["\']([^"\']+)["\']',
        r'default["\']?\s*:\s*["\']([a-zA-Z][a-zA-Z0-9_-]+\.(?:safetensors|ckpt|pt|pth|bin|onnx))["\']',
        r'(?:checkpoint|ckpt|lora|vae|embedding)[_-]?(?:file|path|name)["\']?\s*:\s*["\']([^"\']+)["\']',
    ]
}


class ReferenceExtractor:
    """å¼•ç”¨æå–å¼•æ“"""

    def __init__(self):
        self.custom_nodes_dir = get_custom_nodes_dir()
        self.extracted_references = []

    def extract_all_references(self, node_dirs: List[Path]) -> Dict[str, List[ModelReference]]:
        """
        ä»æ‰€æœ‰æŒ‡å®šèŠ‚ç‚¹ç›®å½•æå–å¼•ç”¨

        Args:
            node_dirs: èŠ‚ç‚¹ç›®å½•åˆ—è¡¨

        Returns:
            Dict[str, List[ModelReference]]: æŒ‰èŠ‚ç‚¹ååˆ†ç»„çš„å¼•ç”¨åˆ—è¡¨
        """
        print("ğŸ” å¼€å§‹æå–æ¨¡å‹å¼•ç”¨...")

        all_references = {}

        for node_dir in node_dirs:
            # è·³è¿‡ComfyModelCleanerè‡ªèº«ï¼Œé¿å…è‡ªå¼•ç”¨
            if 'ComfyModelCleaner' in node_dir.name:
                print(f"  è·³è¿‡è‡ªèº«èŠ‚ç‚¹: {node_dir.name}")
                continue

            print(f"  åˆ†æèŠ‚ç‚¹: {node_dir.name}")

            node_references = []

            # ä»Pythonæ–‡ä»¶æå–
            python_refs = self.extract_from_python_files(node_dir)
            node_references.extend(python_refs)

            # ä»é…ç½®æ–‡ä»¶æå–
            config_refs = self.extract_from_config_files(node_dir)
            node_references.extend(config_refs)

            # ä»ç¤ºä¾‹å·¥ä½œæµæå–
            workflow_refs = self.extract_from_example_workflows(node_dir)
            node_references.extend(workflow_refs)

            # ä»æ–‡æ¡£æå–
            doc_refs = self.extract_from_documentation(node_dir)
            node_references.extend(doc_refs)

            if node_references:
                all_references[node_dir.name] = node_references
                print(f"    å‘ç° {len(node_references)} ä¸ªå¼•ç”¨")
            else:
                print(f"    æ— å¼•ç”¨å‘ç°")

        print(f"âœ… å¼•ç”¨æå–å®Œæˆï¼Œå…± {sum(len(refs) for refs in all_references.values())} ä¸ªå¼•ç”¨")
        return all_references

    def extract_from_python_files(self, node_dir: Path) -> List[ModelReference]:
        """
        ä»Pythonæ–‡ä»¶æå–æ¨¡å‹å¼•ç”¨

        Args:
            node_dir: èŠ‚ç‚¹ç›®å½•

        Returns:
            List[ModelReference]: å¼•ç”¨åˆ—è¡¨
        """
        references = []

        # æŸ¥æ‰¾Pythonæ–‡ä»¶
        python_files = list(node_dir.rglob('*.py'))[:20]  # é™åˆ¶æ–‡ä»¶æ•°é‡

        for py_file in python_files:
            try:
                file_refs = self._extract_from_python_file(py_file)
                references.extend(file_refs)
            except Exception as e:
                print(f"    âŒ åˆ†æPythonæ–‡ä»¶å¤±è´¥ {py_file.name}: {e}")
                continue

        return references

    @safe_file_operation
    def _extract_from_python_file(self, py_file: Path) -> List[ModelReference]:
        """ä»å•ä¸ªPythonæ–‡ä»¶æå–å¼•ç”¨"""
        references = []

        with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()

        for line_num, line in enumerate(lines, 1):
            line_refs = self._extract_references_from_line(
                line, str(py_file), 'python', line_num
            )
            references.extend(line_refs)

        return references

    def extract_from_config_files(self, node_dir: Path) -> List[ModelReference]:
        """
        ä»é…ç½®æ–‡ä»¶æå–å¼•ç”¨

        Args:
            node_dir: èŠ‚ç‚¹ç›®å½•

        Returns:
            List[ModelReference]: å¼•ç”¨åˆ—è¡¨
        """
        references = []

        # é…ç½®æ–‡ä»¶æ¨¡å¼
        config_patterns = ['*.json', '*.yaml', '*.yml', '*.toml', '*.cfg', '*.ini']

        for pattern in config_patterns:
            for config_file in node_dir.glob(pattern):
                try:
                    file_refs = self._extract_from_config_file(config_file)
                    references.extend(file_refs)
                except Exception as e:
                    print(f"    âŒ åˆ†æé…ç½®æ–‡ä»¶å¤±è´¥ {config_file.name}: {e}")
                    continue

        return references

    @safe_file_operation
    def _extract_from_config_file(self, config_file: Path) -> List[ModelReference]:
        """ä»å•ä¸ªé…ç½®æ–‡ä»¶æå–å¼•ç”¨"""
        references = []

        with open(config_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        # å°è¯•è§£æä¸ºJSON/YAML
        try:
            if config_file.suffix.lower() == '.json':
                data = json.loads(content)
                refs = self._extract_from_structured_data(data, str(config_file), 'config')
                references.extend(refs)
            elif config_file.suffix.lower() in ['.yaml', '.yml']:
                data = yaml.safe_load(content)
                refs = self._extract_from_structured_data(data, str(config_file), 'config')
                references.extend(refs)
        except Exception:
            pass

        # æ–‡æœ¬æ¨¡å¼æå–
        lines = content.split('\n')
        for line_num, line in enumerate(lines, 1):
            line_refs = self._extract_references_from_line(
                line, str(config_file), 'config', line_num
            )
            references.extend(line_refs)

        return references

    def extract_from_example_workflows(self, node_dir: Path) -> List[ModelReference]:
        """
        ä»ç¤ºä¾‹å·¥ä½œæµæå–å¼•ç”¨

        Args:
            node_dir: èŠ‚ç‚¹ç›®å½•

        Returns:
            List[ModelReference]: å¼•ç”¨åˆ—è¡¨
        """
        references = []

        # æŸ¥æ‰¾ç¤ºä¾‹å·¥ä½œæµæ–‡ä»¶
        workflow_patterns = ['*example*.json', '*workflow*.json', '*demo*.json']

        for pattern in workflow_patterns:
            for workflow_file in node_dir.rglob(pattern):
                try:
                    file_refs = self._extract_from_workflow_file(workflow_file)
                    references.extend(file_refs)
                except Exception as e:
                    print(f"    âŒ åˆ†æå·¥ä½œæµæ–‡ä»¶å¤±è´¥ {workflow_file.name}: {e}")
                    continue

        return references

    @safe_file_operation
    def _extract_from_workflow_file(self, workflow_file: Path) -> List[ModelReference]:
        """ä»å·¥ä½œæµæ–‡ä»¶æå–å¼•ç”¨"""
        references = []

        with open(workflow_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        try:
            data = json.loads(content)
            refs = self._extract_from_structured_data(data, str(workflow_file), 'workflow')
            references.extend(refs)
        except json.JSONDecodeError:
            # æ–‡æœ¬æ¨¡å¼æå–
            lines = content.split('\n')
            for line_num, line in enumerate(lines, 1):
                line_refs = self._extract_references_from_line(
                    line, str(workflow_file), 'workflow', line_num
                )
                references.extend(line_refs)

        return references

    def extract_from_documentation(self, node_dir: Path) -> List[ModelReference]:
        """
        ä»READMEç­‰æ–‡æ¡£æå–å¼•ç”¨

        Args:
            node_dir: èŠ‚ç‚¹ç›®å½•

        Returns:
            List[ModelReference]: å¼•ç”¨åˆ—è¡¨
        """
        references = []

        # æ–‡æ¡£æ–‡ä»¶æ¨¡å¼
        doc_patterns = ['README*', '*.md', '*.rst', '*.txt', 'INSTALL*', 'SETUP*']

        for pattern in doc_patterns:
            for doc_file in node_dir.glob(pattern):
                try:
                    file_refs = self._extract_from_doc_file(doc_file)
                    references.extend(file_refs)
                except Exception as e:
                    print(f"    âŒ åˆ†ææ–‡æ¡£æ–‡ä»¶å¤±è´¥ {doc_file.name}: {e}")
                    continue

        return references

    @safe_file_operation
    def _extract_from_doc_file(self, doc_file: Path) -> List[ModelReference]:
        """ä»æ–‡æ¡£æ–‡ä»¶æå–å¼•ç”¨"""
        references = []

        with open(doc_file, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()

        for line_num, line in enumerate(lines, 1):
            line_refs = self._extract_references_from_line(
                line, str(doc_file), 'documentation', line_num
            )
            references.extend(line_refs)

        return references

    def _extract_references_from_line(self, line: str, source_file: str,
                                     source_type: str, line_number: int) -> List[ModelReference]:
        """
        ä»å•è¡Œæ–‡æœ¬æå–æ¨¡å‹å¼•ç”¨

        Args:
            line: æ–‡æœ¬è¡Œ
            source_file: æºæ–‡ä»¶è·¯å¾„
            source_type: æºæ–‡ä»¶ç±»å‹
            line_number: è¡Œå·

        Returns:
            List[ModelReference]: å¼•ç”¨åˆ—è¡¨
        """
        references = []
        seen_models = set()  # é¿å…é‡å¤æå–

        # åº”ç”¨æ‰€æœ‰å¼•ç”¨æ¨¡å¼
        for pattern_type, patterns in REFERENCE_PATTERNS.items():
            for pattern in patterns:
                matches = re.findall(pattern, line, re.IGNORECASE)
                for match in matches:
                    if match and self._is_valid_model_reference(match):
                        clean_name = self._clean_model_name(match)

                        # é¿å…é‡å¤æå–ç›¸åŒçš„æ¨¡å‹å
                        if clean_name in seen_models:
                            continue
                        seen_models.add(clean_name)

                        confidence = self._calculate_reference_confidence(
                            match, pattern_type, source_type
                        )

                        ref = ModelReference(
                            model_name=clean_name,
                            source_file=source_file,
                            source_type=source_type,
                            line_number=line_number,
                            context=line.strip(),
                            confidence=confidence
                        )
                        references.append(ref)

        return references

    def _extract_from_structured_data(self, data: Any, source_file: str,
                                    source_type: str) -> List[ModelReference]:
        """
        ä»ç»“æ„åŒ–æ•°æ®ï¼ˆJSON/YAMLï¼‰æå–å¼•ç”¨

        Args:
            data: ç»“æ„åŒ–æ•°æ®
            source_file: æºæ–‡ä»¶è·¯å¾„
            source_type: æºæ–‡ä»¶ç±»å‹

        Returns:
            List[ModelReference]: å¼•ç”¨åˆ—è¡¨
        """
        references = []

        def extract_recursive(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    current_path = f"{path}.{key}" if path else key

                    # æ£€æŸ¥é”®åæ˜¯å¦è¡¨ç¤ºæ¨¡å‹
                    if self._is_model_key(key) and isinstance(value, str):
                        if self._is_valid_model_reference(value):
                            confidence = self._calculate_reference_confidence(
                                value, 'structured_data', source_type
                            )

                            ref = ModelReference(
                                model_name=self._clean_model_name(value),
                                source_file=source_file,
                                source_type=source_type,
                                line_number=None,
                                context=f"{current_path}: {value}",
                                confidence=confidence
                            )
                            references.append(ref)

                    # é€’å½’å¤„ç†å€¼
                    extract_recursive(value, current_path)

            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    extract_recursive(item, f"{path}[{i}]")

            elif isinstance(obj, str):
                # æ£€æŸ¥å­—ç¬¦ä¸²å€¼æ˜¯å¦åŒ…å«æ¨¡å‹å¼•ç”¨
                # åªæœ‰åœ¨æ˜ç¡®çš„æ¨¡å‹ä¸Šä¸‹æ–‡ä¸­æ‰æå–å­—ç¬¦ä¸²å€¼
                if (path and any(model_key in path.lower() for model_key in
                    ['model', 'checkpoint', 'ckpt', 'lora', 'vae', 'embedding']) and
                    self._is_valid_model_reference(obj)):

                    confidence = self._calculate_reference_confidence(
                        obj, 'string_value', source_type
                    )

                    ref = ModelReference(
                        model_name=self._clean_model_name(obj),
                        source_file=source_file,
                        source_type=source_type,
                        line_number=None,
                        context=f"{path}: {obj}",
                        confidence=confidence
                    )
                    references.append(ref)

        extract_recursive(data)
        return references

    def _is_valid_model_reference(self, text: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ¨¡å‹å¼•ç”¨

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            bool: æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ¨¡å‹å¼•ç”¨
        """
        if not text or len(text) < 3:
            return False

        # æ’é™¤æ˜æ˜¾çš„éæ¨¡å‹å¼•ç”¨
        invalid_patterns = [
            r'^https?://',  # URL
            r'^[a-zA-Z]:\\',  # Windowsè·¯å¾„
            r'^/[a-zA-Z]',  # Unixç»å¯¹è·¯å¾„
            r'^\$\{',  # å˜é‡å¼•ç”¨
            r'^[0-9]+$',  # çº¯æ•°å­—
            r'^(true|false|null|none)$',  # å¸ƒå°”å€¼å’Œç©ºå€¼
        ]

        for pattern in invalid_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                return False

        text_lower = text.lower()

        # æ’é™¤ComfyUIæ¨¡å‹ç›®å½•å®šä¹‰ï¼ˆè¿™äº›æ˜¯ç›®å½•ç±»å‹ï¼Œä¸æ˜¯å…·ä½“çš„æ¨¡å‹å¼•ç”¨ï¼‰
        comfyui_directory_types = {
            'checkpoints', 'loras', 'embeddings', 'vae', 'controlnet',
            'clip', 'unet', 'diffusers', 'upscale_models', 'gligen',
            'style_models', 'clip_vision', 'facedetection', 'facerestore_models',
            'sams', 'mmdets', 'onnx', 'custom', 'animatediff_models',
            'photomaker', 'instantid', 'ipadapter', 'layerstyle', 'hypernetworks'
        }

        if text_lower in comfyui_directory_types:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶æ‰©å±•åæˆ–æ¨¡å‹ç›¸å…³å…³é”®è¯
        model_indicators = [
            '.safetensors', '.ckpt', '.pt', '.pth', '.bin', '.onnx',
            'checkpoint', 'model', 'lora', 'embedding', 'vae',
            'segformer', 'clip', 'vit', 'sam', 'controlnet'  # æ·»åŠ ç‰¹å®šæ¨¡å‹ç±»å‹
        ]

        # å¦‚æœåŒ…å«æ¨¡å‹æŒ‡ç¤ºç¬¦ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ•ˆå¼•ç”¨
        if any(indicator in text_lower for indicator in model_indicators):
            return True

        # å¯¹äºæ²¡æœ‰æ˜æ˜¾æŒ‡ç¤ºç¬¦çš„æ–‡æœ¬ï¼Œæ£€æŸ¥æ˜¯å¦ç¬¦åˆæ¨¡å‹åç§°æ¨¡å¼
        # æ¨¡å‹åç§°é€šå¸¸æ˜¯å­—æ¯å¼€å¤´ï¼ŒåŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦
        if re.match(r'^[a-zA-Z][a-zA-Z0-9_-]*$', text) and len(text) >= 5:  # æé«˜æœ€å°é•¿åº¦è¦æ±‚
            # æ’é™¤å¸¸è§çš„éæ¨¡å‹è¯æ±‡ï¼ˆå¤§å¹…æ‰©å±•æ’é™¤åˆ—è¡¨ï¼‰
            common_words = {
                # åŸºç¡€è¯æ±‡
                'default', 'none', 'auto', 'true', 'false', 'yes', 'no',
                'input', 'output', 'image', 'text', 'string', 'int', 'float',
                'width', 'height', 'size', 'scale', 'step', 'steps',
                # ç¼–ç¨‹ç›¸å…³
                'class', 'function', 'method', 'return', 'import', 'from',
                'self', 'super', 'init', 'main', 'test', 'debug', 'error',
                'warning', 'info', 'config', 'settings', 'options', 'params',
                # UIç›¸å…³
                'button', 'label', 'title', 'description', 'tooltip', 'help',
                'menu', 'dialog', 'window', 'panel', 'tab', 'page', 'view',
                'layout', 'style', 'theme', 'color', 'font', 'icon',
                # é€šç”¨å±æ€§
                'name', 'type', 'value', 'data', 'item', 'list', 'dict',
                'array', 'object', 'element', 'node', 'path', 'file',
                'folder', 'directory', 'extension', 'format', 'version',
                # ComfyUIç›¸å…³ä½†éæ¨¡å‹
                'comfy', 'comfyui', 'workflow', 'queue', 'prompt', 'execute',
                'preview', 'progress', 'status', 'result', 'output_dir',
                # å¸¸è§çš„çŸ­è¯
                'id', 'key', 'val', 'src', 'dst', 'tmp', 'temp', 'cache',
                'log', 'msg', 'err', 'ok', 'run', 'stop', 'start', 'end'
            }
            if text_lower not in common_words:
                # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœé•¿åº¦å¾ˆçŸ­ä¸”ä¸åŒ…å«æ¨¡å‹ç‰¹å¾ï¼Œåˆ™æ‹’ç»
                if len(text) < 8 and not any(char.isdigit() for char in text):
                    return False
                return True

        return False

    def _is_model_key(self, key: str) -> bool:
        """
        åˆ¤æ–­é”®åæ˜¯å¦è¡¨ç¤ºæ¨¡å‹

        Args:
            key: é”®å

        Returns:
            bool: æ˜¯å¦æ˜¯æ¨¡å‹é”®
        """
        model_keys = [
            'model', 'checkpoint', 'ckpt', 'model_name', 'checkpoint_name',
            'ckpt_name', 'lora', 'lora_name', 'vae', 'vae_name',
            'embedding', 'embedding_name', 'controlnet', 'control_net_name'
        ]

        key_lower = key.lower()
        return any(model_key in key_lower for model_key in model_keys)

    def _clean_model_name(self, name: str) -> str:
        """
        æ¸…ç†æ¨¡å‹åç§°

        Args:
            name: åŸå§‹åç§°

        Returns:
            str: æ¸…ç†åçš„åç§°
        """
        # ç§»é™¤è·¯å¾„éƒ¨åˆ†ï¼Œåªä¿ç•™æ–‡ä»¶å
        clean_name = Path(name).name

        # ç§»é™¤å¸¸è§çš„å‰ç¼€å’Œåç¼€
        clean_name = re.sub(r'^(models?[/\\])', '', clean_name, flags=re.IGNORECASE)
        clean_name = re.sub(r'\.(safetensors|ckpt|pt|pth|bin|onnx)$', '', clean_name, flags=re.IGNORECASE)

        return clean_name.strip()

    def _calculate_reference_confidence(self, reference: str, pattern_type: str,
                                      source_type: str) -> float:
        """
        è®¡ç®—å¼•ç”¨çš„ç½®ä¿¡åº¦

        Args:
            reference: å¼•ç”¨æ–‡æœ¬
            pattern_type: æ¨¡å¼ç±»å‹
            source_type: æºæ–‡ä»¶ç±»å‹

        Returns:
            float: ç½®ä¿¡åº¦ (0.0 - 1.0)
        """
        confidence = 0.5  # åŸºç¡€ç½®ä¿¡åº¦

        # æ ¹æ®æ¨¡å¼ç±»å‹è°ƒæ•´
        pattern_weights = {
            'exact_file_reference': 0.9,
            'directory_reference': 0.7,
            'model_name_patterns': 0.8,
            'structured_data': 0.8,
            'string_value': 0.6
        }
        confidence = pattern_weights.get(pattern_type, 0.5)

        # æ ¹æ®æºæ–‡ä»¶ç±»å‹è°ƒæ•´
        source_weights = {
            'python': 1.0,
            'config': 0.9,
            'workflow': 0.8,
            'documentation': 0.6
        }
        confidence *= source_weights.get(source_type, 0.7)

        # æ ¹æ®å¼•ç”¨å†…å®¹è°ƒæ•´
        if any(ext in reference.lower() for ext in ['.safetensors', '.ckpt']):
            confidence += 0.1

        if 'default' in reference.lower() or 'example' in reference.lower():
            confidence -= 0.1

        return max(0.0, min(1.0, confidence))
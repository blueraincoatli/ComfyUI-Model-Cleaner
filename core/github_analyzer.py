"""
GitHubå¢å¼ºåˆ†æå™¨ - ComfyModelCleaner V2.0

å¯é€‰çš„GitHubä»“åº“ä¿¡æ¯è·å–ï¼Œå¢å¼ºæ¨¡å‹å¼•ç”¨æ£€æµ‹ã€‚
"""

import re
import json
import time
import urllib.request
import urllib.parse
from pathlib import Path
from typing import Dict, List, Set, Optional, Any
from dataclasses import dataclass

from .utils import safe_file_operation


@dataclass
class GitHubRepoInfo:
    """GitHubä»“åº“ä¿¡æ¯"""
    url: str
    name: str
    description: str
    readme_content: str
    model_references: List[str]
    last_updated: float


class GitHubCache:
    """GitHubä¿¡æ¯ç¼“å­˜"""

    def __init__(self, cache_duration: int = 24*3600):  # 24å°æ—¶ç¼“å­˜
        self.cache_duration = cache_duration
        self.cache_file = Path("github_cache.json")
        self.cache_data = self._load_cache()

    def _load_cache(self) -> Dict[str, Any]:
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception:
            pass
        return {}

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜GitHubç¼“å­˜å¤±è´¥: {e}")

    def get_cached_info(self, repo_url: str) -> Optional[GitHubRepoInfo]:
        """è·å–ç¼“å­˜çš„ä»“åº“ä¿¡æ¯"""
        if repo_url not in self.cache_data:
            return None

        cached = self.cache_data[repo_url]

        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if time.time() - cached.get('timestamp', 0) > self.cache_duration:
            del self.cache_data[repo_url]
            self._save_cache()
            return None

        try:
            return GitHubRepoInfo(
                url=cached['url'],
                name=cached['name'],
                description=cached['description'],
                readme_content=cached['readme_content'],
                model_references=cached['model_references'],
                last_updated=cached['timestamp']
            )
        except KeyError:
            return None

    def cache_info(self, repo_url: str, info: GitHubRepoInfo):
        """ç¼“å­˜ä»“åº“ä¿¡æ¯"""
        self.cache_data[repo_url] = {
            'url': info.url,
            'name': info.name,
            'description': info.description,
            'readme_content': info.readme_content,
            'model_references': info.model_references,
            'timestamp': time.time()
        }
        self._save_cache()


class GitHubAnalyzer:
    """GitHubåˆ†æå™¨"""

    def __init__(self, enable_cache: bool = True):
        self.cache = GitHubCache() if enable_cache else None
        self.timeout = 10  # è¯·æ±‚è¶…æ—¶æ—¶é—´

    def analyze_node_repositories(self, node_dirs: List[Path]) -> Dict[str, GitHubRepoInfo]:
        """
        åˆ†æèŠ‚ç‚¹çš„GitHubä»“åº“ä¿¡æ¯

        Args:
            node_dirs: èŠ‚ç‚¹ç›®å½•åˆ—è¡¨

        Returns:
            Dict[str, GitHubRepoInfo]: èŠ‚ç‚¹ååˆ°ä»“åº“ä¿¡æ¯çš„æ˜ å°„
        """
        print("ğŸŒ å¼€å§‹GitHubä»“åº“åˆ†æ...")

        repo_infos = {}

        for node_dir in node_dirs:
            try:
                repo_url = self.extract_repo_info(node_dir)
                if repo_url:
                    print(f"  åˆ†æä»“åº“: {node_dir.name} -> {repo_url}")

                    # æ£€æŸ¥ç¼“å­˜
                    if self.cache:
                        cached_info = self.cache.get_cached_info(repo_url)
                        if cached_info:
                            repo_infos[node_dir.name] = cached_info
                            print(f"    ä½¿ç”¨ç¼“å­˜ä¿¡æ¯")
                            continue

                    # è·å–ä»“åº“ä¿¡æ¯
                    repo_info = self.fetch_repo_info(repo_url)
                    if repo_info:
                        repo_infos[node_dir.name] = repo_info

                        # ç¼“å­˜ä¿¡æ¯
                        if self.cache:
                            self.cache.cache_info(repo_url, repo_info)

                        print(f"    å‘ç° {len(repo_info.model_references)} ä¸ªæ¨¡å‹å¼•ç”¨")
                    else:
                        print(f"    è·å–ä»“åº“ä¿¡æ¯å¤±è´¥")
                else:
                    print(f"  {node_dir.name}: æœªæ‰¾åˆ°GitHubä»“åº“")

            except Exception as e:
                print(f"  âŒ åˆ†æ {node_dir.name} å¤±è´¥: {e}")
                continue

        print(f"âœ… GitHubåˆ†æå®Œæˆï¼Œåˆ†æäº† {len(repo_infos)} ä¸ªä»“åº“")
        return repo_infos

    def extract_repo_info(self, node_dir: Path) -> Optional[str]:
        """
        ä»èŠ‚ç‚¹ç›®å½•æå–GitHubä»“åº“ä¿¡æ¯

        Args:
            node_dir: èŠ‚ç‚¹ç›®å½•

        Returns:
            Optional[str]: GitHubä»“åº“URL
        """
        # æ£€æŸ¥.git/configæ–‡ä»¶
        git_config = node_dir / ".git" / "config"
        if git_config.exists():
            try:
                repo_url = self._extract_from_git_config(git_config)
                if repo_url:
                    return repo_url
            except Exception:
                pass

        # æ£€æŸ¥package.json
        package_json = node_dir / "package.json"
        if package_json.exists():
            try:
                repo_url = self._extract_from_package_json(package_json)
                if repo_url:
                    return repo_url
            except Exception:
                pass

        # æ£€æŸ¥READMEæ–‡ä»¶
        readme_files = list(node_dir.glob("README*")) + list(node_dir.glob("readme*"))
        for readme_file in readme_files:
            try:
                repo_url = self._extract_from_readme(readme_file)
                if repo_url:
                    return repo_url
            except Exception:
                continue

        return None

    @safe_file_operation
    def _extract_from_git_config(self, git_config: Path) -> Optional[str]:
        """ä»gité…ç½®æ–‡ä»¶æå–ä»“åº“URL"""
        with open(git_config, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        # æŸ¥æ‰¾origin remoteçš„URL
        patterns = [
            r'url\s*=\s*https://github\.com/([^/]+/[^/\s]+)',
            r'url\s*=\s*git@github\.com:([^/]+/[^/\s]+)\.git'
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                repo_path = match.group(1)
                return f"https://github.com/{repo_path}"

        return None

    @safe_file_operation
    def _extract_from_package_json(self, package_json: Path) -> Optional[str]:
        """ä»package.jsonæå–ä»“åº“URL"""
        with open(package_json, 'r', encoding='utf-8', errors='ignore') as f:
            data = json.load(f)

        # æ£€æŸ¥repositoryå­—æ®µ
        repo = data.get('repository', {})
        if isinstance(repo, dict):
            url = repo.get('url', '')
        elif isinstance(repo, str):
            url = repo
        else:
            return None

        # æ ‡å‡†åŒ–GitHub URL
        if 'github.com' in url:
            match = re.search(r'github\.com[:/]([^/]+/[^/\s]+)', url)
            if match:
                repo_path = match.group(1).rstrip('.git')
                return f"https://github.com/{repo_path}"

        return None

    @safe_file_operation
    def _extract_from_readme(self, readme_file: Path) -> Optional[str]:
        """ä»READMEæ–‡ä»¶æå–GitHub URL"""
        with open(readme_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        # æŸ¥æ‰¾GitHubé“¾æ¥
        patterns = [
            r'https://github\.com/([^/\s]+/[^/\s]+)',
            r'\[.*?\]\(https://github\.com/([^/\s]+/[^/\s]+)\)',
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                repo_path = match.group(1).rstrip('.git')
                return f"https://github.com/{repo_path}"

        return None

    def fetch_repo_info(self, repo_url: str) -> Optional[GitHubRepoInfo]:
        """
        è·å–GitHubä»“åº“ä¿¡æ¯

        Args:
            repo_url: ä»“åº“URL

        Returns:
            Optional[GitHubRepoInfo]: ä»“åº“ä¿¡æ¯
        """
        try:
            # æå–ä»“åº“è·¯å¾„
            match = re.search(r'github\.com/([^/]+/[^/]+)', repo_url)
            if not match:
                return None

            repo_path = match.group(1)

            # è·å–READMEå†…å®¹
            readme_url = f"https://raw.githubusercontent.com/{repo_path}/main/README.md"
            readme_content = self._fetch_url_content(readme_url)

            if not readme_content:
                # å°è¯•masteråˆ†æ”¯
                readme_url = f"https://raw.githubusercontent.com/{repo_path}/master/README.md"
                readme_content = self._fetch_url_content(readme_url)

            if not readme_content:
                readme_content = ""

            # æå–æ¨¡å‹å¼•ç”¨
            model_references = self.extract_model_references_from_readme(readme_content)

            return GitHubRepoInfo(
                url=repo_url,
                name=repo_path.split('/')[-1],
                description="",  # å¯ä»¥é€šè¿‡GitHub APIè·å–ï¼Œä½†éœ€è¦è®¤è¯
                readme_content=readme_content,
                model_references=model_references,
                last_updated=time.time()
            )

        except Exception as e:
            print(f"è·å–GitHubä»“åº“ä¿¡æ¯å¤±è´¥ {repo_url}: {e}")
            return None

    def _fetch_url_content(self, url: str) -> Optional[str]:
        """è·å–URLå†…å®¹"""
        try:
            req = urllib.request.Request(url)
            req.add_header('User-Agent', 'ComfyModelCleaner/2.0')

            with urllib.request.urlopen(req, timeout=self.timeout) as response:
                content = response.read().decode('utf-8', errors='ignore')
                return content

        except Exception:
            return None

    def extract_model_references_from_readme(self, readme_content: str) -> List[str]:
        """
        ä»READMEå†…å®¹æå–æ¨¡å‹å¼•ç”¨ - å¢å¼ºç‰ˆ

        Args:
            readme_content: READMEå†…å®¹

        Returns:
            List[str]: æ¨¡å‹å¼•ç”¨åˆ—è¡¨
        """
        model_references = []

        # åŸºæœ¬æ¨¡å‹æ–‡ä»¶æ‰©å±•åæ¨¡å¼
        basic_patterns = [
            r'([a-zA-Z0-9_.-]+\.(?:safetensors|ckpt|pt|pth|bin|onnx))',
            r'models/([a-zA-Z0-9_/-]+)',
            r'download.*?([a-zA-Z0-9_.-]+\.(?:safetensors|ckpt))',
            r'place.*?in.*?models/([a-zA-Z0-9_-]+)',
        ]

        # å¢å¼ºçš„æ¨¡å‹åç§°æ¨¡å¼ - é’ˆå¯¹GitHubé¡µé¢å¸¸è§æ ¼å¼
        enhanced_patterns = [
            # åŒ¹é…é“¾æ¥ä¸­çš„æ¨¡å‹å (å¦‚ [ip-adapter_sd15.safetensors](url))
            r'\[([a-zA-Z0-9_.-]+\.(?:safetensors|ckpt|pt|pth|bin|onnx))\]',
            # åŒ¹é…ä»£ç å—ä¸­çš„æ¨¡å‹å
            r'`([a-zA-Z0-9_.-]+\.(?:safetensors|ckpt|pt|pth|bin|onnx))`',
            # åŒ¹é…åˆ—è¡¨é¡¹ä¸­çš„æ¨¡å‹å
            r'[â€¢\-\*]\s*([a-zA-Z0-9_.-]+\.(?:safetensors|ckpt|pt|pth|bin|onnx))',
            # åŒ¹é…"download and rename"æ¨¡å¼
            r'download\s+and\s+rename.*?([a-zA-Z0-9_.-]+\.(?:safetensors|ckpt|pt|pth|bin))',
            # åŒ¹é…HuggingFaceé“¾æ¥ä¸­çš„æ¨¡å‹å
            r'huggingface\.co/[^/]+/[^/]+/[^/]+/([a-zA-Z0-9_.-]+\.(?:safetensors|ckpt|pt|pth|bin))',
            # åŒ¹é…è·¯å¾„æ ¼å¼çš„æ¨¡å‹å¼•ç”¨
            r'/ComfyUI/models/[^/]+/([a-zA-Z0-9_.-]+\.(?:safetensors|ckpt|pt|pth|bin))',
            # åŒ¹é…ä¸å¸¦æ‰©å±•åçš„æ¨¡å‹åï¼ˆåœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­ï¼‰
            r'(?:ip-adapter|clip|vit|model)[-_]([a-zA-Z0-9_.-]+)(?:\.safetensors|\.ckpt|\.pt|\.pth|\.bin)?',
            # åŒ¹é…è¡¨æ ¼ä¸­çš„æ¨¡å‹å
            r'\|\s*([a-zA-Z0-9_.-]+\.(?:safetensors|ckpt|pt|pth|bin))\s*\|',
        ]

        # ç‰¹æ®Šçš„æ¨¡å‹åç§°æ¨¡å¼ï¼ˆä¸ä¾èµ–æ‰©å±•åï¼‰
        contextual_patterns = [
            # IP-Adapterç›¸å…³æ¨¡å‹
            r'(ip-adapter[a-zA-Z0-9_.-]*)',
            r'(clip-vit[a-zA-Z0-9_.-]*)',
            # ControlNetç›¸å…³æ¨¡å‹
            r'(control[a-zA-Z0-9_.-]*)',
            # VAEç›¸å…³æ¨¡å‹
            r'(vae[a-zA-Z0-9_.-]*)',
            # å…¶ä»–å¸¸è§æ¨¡å‹å‰ç¼€
            r'(sam[a-zA-Z0-9_.-]*)',
            r'(yolo[a-zA-Z0-9_.-]*)',
            r'(resnet[a-zA-Z0-9_.-]*)',
        ]

        # åº”ç”¨åŸºæœ¬æ¨¡å¼
        for pattern in basic_patterns:
            matches = re.findall(pattern, readme_content, re.IGNORECASE)
            for match in matches:
                if match and len(match) > 3:
                    model_references.append(match)

        # åº”ç”¨å¢å¼ºæ¨¡å¼
        for pattern in enhanced_patterns:
            matches = re.findall(pattern, readme_content, re.IGNORECASE)
            for match in matches:
                if match and len(match) > 3:
                    # æ¸…ç†åŒ¹é…ç»“æœ
                    clean_match = match.strip('`[]()').strip()
                    if clean_match:
                        model_references.append(clean_match)

        # åº”ç”¨ä¸Šä¸‹æ–‡æ¨¡å¼ï¼ˆåªåœ¨ç‰¹å®šå…³é”®è¯é™„è¿‘ï¼‰
        lines = readme_content.split('\n')
        for i, line in enumerate(lines):
            line_lower = line.lower()
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹ç›¸å…³å…³é”®è¯
            if any(keyword in line_lower for keyword in ['model', 'download', 'place', 'file', 'checkpoint']):
                for pattern in contextual_patterns:
                    matches = re.findall(pattern, line, re.IGNORECASE)
                    for match in matches:
                        if match and len(match) > 3:
                            model_references.append(match)

        # æ¸…ç†å’Œå»é‡
        cleaned_references = []
        for ref in model_references:
            # ç§»é™¤å¸¸è§çš„éæ¨¡å‹è¯æ±‡
            if not any(exclude in ref.lower() for exclude in ['http', 'www', 'github', 'readme', 'license', 'install']):
                # ç§»é™¤è·¯å¾„å‰ç¼€
                clean_ref = ref.split('/')[-1] if '/' in ref else ref
                if len(clean_ref) > 3:
                    cleaned_references.append(clean_ref)

        return list(set(cleaned_references))
